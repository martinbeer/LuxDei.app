import requests
import pandas as pd
import re
import time
from pathlib import Path
from bs4 import BeautifulSoup

def get_all_author_links():
    """Holt alle Autor-Links von der BKV-Website"""
    print("ğŸ” SAMMLE ALLE AUTOR-LINKS")
    print("=" * 50)
    
    try:
        response = requests.get("https://bkv.unifr.ch/de/works")
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        author_links = {}
        
        # Suche nach allen Autor-Links - sie sind direkt in h5 als Links
        for h5 in soup.find_all('h5'):
            author_name = h5.get_text().strip()
            
            # Finde Link direkt in h5 oder im nÃ¤chsten Element
            link = None
            if h5.find('a'):
                link = h5.find('a')['href']
            else:
                # Schaue in nachfolgenden Elementen
                next_sibling = h5.find_next_sibling()
                while next_sibling and not link:
                    if next_sibling.name == 'p' and next_sibling.find('a'):
                        link = next_sibling.find('a')['href']
                        break
                    next_sibling = next_sibling.find_next_sibling()
            
            if link:
                full_url = "https://bkv.unifr.ch" + link if link.startswith('/') else link
                author_links[author_name] = full_url
                print(f"   ğŸ“ {author_name}: {full_url}")
        
        print(f"\nâœ… {len(author_links)} Autoren gefunden")
        return author_links
        
    except Exception as e:
        print(f"âŒ Fehler: {e}")
        return {}

def get_german_works_from_author(author_name, author_url):
    """Holt deutsche Werke eines Autors"""
    print(f"\nğŸ“– Verarbeite: {author_name}")
    print(f"   ğŸ”— URL: {author_url}")
    
    try:
        time.sleep(1)  # Rate limiting
        response = requests.get(author_url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        german_works = []
        
        # Suche nach Werk-Links
        for link in soup.find_all('a', href=True):
            link_text = link.get_text().strip()
            
            # Filtere deutsche Ãœbersetzungen
            if any(keyword in link_text.lower() for keyword in ['deutsch', 'Ã¼bersetzung', 'bkv']):
                work_url = "https://bkv.unifr.ch" + link['href'] if link['href'].startswith('/') else link['href']
                
                clean_title = clean_work_title(link_text)
                
                german_works.append({
                    'title': clean_title,
                    'url': work_url,
                    'original_text': link_text
                })
                print(f"   ğŸ“„ Deutsches Werk: {clean_title}")
        
        return german_works
        
    except Exception as e:
        print(f"   âŒ Fehler: {e}")
        return []

def clean_work_title(title):
    """Bereinigt Werktitel"""
    # Entferne Ãœbersetzungshinweise
    title = re.sub(r'Ãœbersetzung \([^)]+\):\s*', '', title)
    title = re.sub(r'Kommentar \([^)]+\):\s*', '', title)
    title = re.sub(r'\(BKV[^)]*\)', '', title)
    title = re.sub(r'\s*\([^)]*\)\s*$', '', title)
    title = re.sub(r'\s+', ' ', title)
    return title.strip()

def extract_text_from_work_page(work_url):
    """Extrahiert Text von einer Werk-Seite"""
    try:
        time.sleep(1)
        response = requests.get(work_url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Suche nach dem Haupttext-Bereich
        text_content = []
        
        # Verschiedene mÃ¶gliche Container fÃ¼r den Text
        content_selectors = [
            'div.content',
            'div.text',
            'main',
            'article',
            'div.work-content',
            '.content-body'
        ]
        
        content_found = False
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                # Entferne Navigation und unwichtige Elemente
                for unwanted in content_div(['nav', 'header', 'footer', 'script', 'style']):
                    unwanted.decompose()
                
                # Hole alle TextabsÃ¤tze
                for element in content_div.find_all(['p', 'div'], string=True):
                    text = element.get_text().strip()
                    if len(text) > 30:  # Nur substantielle Texte
                        text_content.append(text)
                
                if text_content:
                    content_found = True
                    break
        
        # Fallback: Hole allen Text
        if not content_found:
            # Entferne unwichtige Elemente
            for unwanted in soup(['nav', 'header', 'footer', 'script', 'style']):
                unwanted.decompose()
            
            text_content = [soup.get_text()]
        
        full_text = '\n'.join(text_content)
        return clean_extracted_text(full_text)
        
    except Exception as e:
        print(f"     âŒ Text-Extraktion fehlgeschlagen: {e}")
        return ""

def clean_extracted_text(text):
    """Bereinigt extrahierten Text"""
    if not text:
        return ""
    
    # Entferne Navigation und Metadaten
    lines_to_remove = [
        r'.*navigation.*',
        r'.*sprache.*',
        r'.*copyright.*',
        r'.*impressum.*',
        r'.*datenschutz.*',
        r'.*cookies.*',
        r'.*start.*werke.*suche.*',
        r'^de\s*en\s*fr$',
        r'.*unifr\.ch.*'
    ]
    
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        
        # Ãœberspringe leere Zeilen
        if not line:
            continue
        
        # Ãœberspringe Navigations-/Metadaten-Zeilen
        skip_line = False
        for pattern in lines_to_remove:
            if re.match(pattern, line, re.IGNORECASE):
                skip_line = True
                break
        
        if not skip_line and len(line) > 10:
            cleaned_lines.append(line)
    
    # FÃ¼ge zusammen und bereinige
    cleaned_text = '\n'.join(cleaned_lines)
    
    # Entferne Ã¼bermÃ¤ÃŸige Leerzeichen
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
    cleaned_text = re.sub(r'\n\s*\n', '\n', cleaned_text)
    
    return cleaned_text.strip()

def create_supabase_entries(text, author_name, work_title):
    """Erstellt Supabase-EintrÃ¤ge aus Text"""
    if not text or len(text) < 200:
        return []
    
    # Teile Text in sinnvolle Abschnitte (ca. 200-400 WÃ¶rter)
    paragraphs = text.split('\n')
    
    entries = []
    current_section = ""
    section_num = 1
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        
        if len(paragraph) < 20:  # Ãœberspringe sehr kurze AbsÃ¤tze
            continue
        
        # Sammle Text fÃ¼r aktuellen Abschnitt
        if current_section:
            current_section += " " + paragraph
        else:
            current_section = paragraph
        
        # Wenn Abschnitt groÃŸ genug ist, speichere ihn
        word_count = len(current_section.split())
        if word_count >= 200:
            
            # Erstelle Eintrag
            entry_id = create_unique_id(author_name, work_title, section_num)
            entries.append({
                'id': entry_id,
                'author': author_name,
                'work_title': work_title,
                'section': section_num,
                'text': current_section.strip(),
                'word_count': word_count,
                'language': 'de'
            })
            
            # Reset fÃ¼r nÃ¤chsten Abschnitt
            current_section = ""
            section_num += 1
    
    # Letzten Abschnitt hinzufÃ¼gen falls groÃŸ genug
    if current_section and len(current_section.split()) >= 50:
        entry_id = create_unique_id(author_name, work_title, section_num)
        entries.append({
            'id': entry_id,
            'author': author_name,
            'work_title': work_title,
            'section': section_num,
            'text': current_section.strip(),
            'word_count': len(current_section.split()),
            'language': 'de'
        })
    
    return entries

def create_unique_id(author, work_title, section):
    """Erstellt eindeutige ID"""
    author_clean = re.sub(r'[^a-zA-Z0-9]', '_', author)
    work_clean = re.sub(r'[^a-zA-Z0-9]', '_', work_title)
    
    if len(work_clean) > 30:
        work_clean = work_clean[:30]
    
    return f"{author_clean}_{work_clean}_{section}"

def save_author_csv(author_name, entries):
    """Speichert CSV fÃ¼r einen Autor"""
    if not entries:
        return False
    
    filename = f"supabase_{author_name.replace(' ', '_')}.csv"
    df = pd.DataFrame(entries)
    df.to_csv(filename, index=False, encoding='utf-8')
    print(f"   ğŸ’¾ Gespeichert: {filename} ({len(entries)} EintrÃ¤ge)")
    return True

def main():
    """Hauptfunktion"""
    print("ğŸš€ BKV KIRCHENVÃ„TER VOLLSTÃ„NDIGER SCRAPER")
    print("=" * 60)
    
    # Hole alle Autor-Links
    author_links = get_all_author_links()
    
    if not author_links:
        print("âŒ Keine Autor-Links gefunden!")
        return
    
    # PrioritÃ¤re Autoren (die noch nicht in CSVs sind)
    priority_authors = [
        "Ambrosius von Mailand",
        "Clemens von Alexandrien", 
        "Gregor von Nyssa",
        "Gregor der Grosse",
        "IrenÃ¤us von Lyon",
        "Johannes Cassianus",
        "Laktanz",
        "Leo der Grosse",
        "Polykarp von Smyrna",
        "Athenagoras von Athen",
        "Cyrill von Jerusalem",
        "Minucius Felix"
    ]
    
    successful_authors = 0
    total_entries = 0
    
    for author in priority_authors:
        if author in author_links:
            try:
                # Hole deutsche Werke
                works = get_german_works_from_author(author, author_links[author])
                
                if not works:
                    print(f"   âŒ Keine deutschen Werke gefunden")
                    continue
                
                # Verarbeite alle Werke des Autors
                author_entries = []
                
                for work in works[:3]:  # Maximal 3 Werke pro Autor
                    print(f"   ğŸ“– Verarbeite: {work['title']}")
                    
                    text = extract_text_from_work_page(work['url'])
                    
                    if text and len(text) > 500:
                        entries = create_supabase_entries(text, author, work['title'])
                        author_entries.extend(entries)
                        print(f"     âœ… {len(entries)} Abschnitte erstellt")
                    else:
                        print(f"     âŒ Zu wenig Text ({len(text)} Zeichen)")
                    
                    time.sleep(2)  # Rate limiting
                
                # Speichere CSV fÃ¼r diesen Autor
                if author_entries:
                    if save_author_csv(author, author_entries):
                        successful_authors += 1
                        total_entries += len(author_entries)
                        print(f"   ğŸ‰ {author}: {len(author_entries)} EintrÃ¤ge gespeichert")
                
            except Exception as e:
                print(f"   âŒ Fehler bei {author}: {e}")
                continue
        else:
            print(f"\nâŒ {author} nicht in Autor-Links gefunden")
    
    # Finale Statistiken
    print(f"\n" + "=" * 60)
    print("ğŸ‰ SCRAPING ABGESCHLOSSEN!")
    print("=" * 60)
    print(f"ğŸ“Š Erfolgreiche Autoren: {successful_authors}")
    print(f"ğŸ“Š Neue EintrÃ¤ge: {total_entries}")

if __name__ == "__main__":
    main()
