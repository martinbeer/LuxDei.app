import requests
import pandas as pd
import re
import time
from pathlib import Path
from bs4 import BeautifulSoup
import os

def get_specific_author_links():
    """Definiert spezifische Autor-Links f√ºr die noch fehlenden Kirchenv√§ter"""
    print("üîç DEFINIERE SPEZIFISCHE AUTOR-LINKS")
    print("=" * 50)
    
    # Nur die wichtigsten Kirchenv√§ter, die noch fehlen
    author_links = {
        "Ambrosius von Mailand": "https://bkv.unifr.ch/de/works/102",
        "Clemens von Alexandrien": "https://bkv.unifr.ch/de/works/17",
        "Gregor von Nyssa": "https://bkv.unifr.ch/de/works/109",
        "Gregor der Grosse": "https://bkv.unifr.ch/de/works/89",
        "Iren√§us von Lyon": "https://bkv.unifr.ch/de/works/15",
        "Johannes Cassianus": "https://bkv.unifr.ch/de/works/80",
        "Laktanz": "https://bkv.unifr.ch/de/works/18",
        "Leo der Grosse": "https://bkv.unifr.ch/de/works/86",
        "Polykarp von Smyrna": "https://bkv.unifr.ch/de/works/6",
        "Athenagoras von Athen": "https://bkv.unifr.ch/de/works/14",
        "Cyrill von Jerusalem": "https://bkv.unifr.ch/de/works/31",
        "Minucius Felix": "https://bkv.unifr.ch/de/works/16"
    }
    
    for author, url in author_links.items():
        print(f"   üìù {author}: {url}")
    
    print(f"\n‚úÖ {len(author_links)} Autoren definiert")
    return author_links

def get_german_works_from_author(author_name, author_url):
    """Holt deutsche Werke eines Autors"""
    print(f"\nüìñ Verarbeite: {author_name}")
    print(f"   üîó URL: {author_url}")
    
    try:
        time.sleep(1)  # Rate limiting
        response = requests.get(author_url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        works = []
        
        # Finde alle Werk-Links (verschiedene Strukturen ausprobieren)
        for link in soup.find_all('a', href=True):
            href = link['href']
            
            # Nur Links zu deutschen Werken
            if '/de/works/' in href and href != author_url:
                title = link.get_text().strip()
                
                # √úberspringe sehr kurze oder leere Titel
                if len(title) < 3 or title.lower() in ['mehr', 'details', 'weiter']:
                    continue
                
                full_url = href if href.startswith('http') else "https://bkv.unifr.ch" + href
                works.append({
                    'title': title,
                    'url': full_url
                })
                print(f"      üìÑ {title}: {full_url}")
        
        # Wenn keine Werke gefunden, versuche andere Strukturen
        if not works:
            # Suche in Listen
            for li in soup.find_all('li'):
                if li.find('a'):
                    link = li.find('a')
                    href = link['href']
                    title = link.get_text().strip()
                    
                    if '/de/works/' in href and len(title) > 3:
                        full_url = href if href.startswith('http') else "https://bkv.unifr.ch" + href
                        works.append({
                            'title': title,
                            'url': full_url
                        })
                        print(f"      üìÑ {title}: {full_url}")
        
        print(f"   ‚úÖ {len(works)} deutsche Werke gefunden")
        return works
        
    except Exception as e:
        print(f"   ‚ùå Fehler: {e}")
        return []

def extract_text_from_work_page(work_url):
    """Extrahiert Text von einer Werk-Seite"""
    print(f"     üìÑ Extrahiere Text von: {work_url}")
    
    try:
        time.sleep(1)  # Rate limiting
        response = requests.get(work_url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Entferne Navigation, Header, Footer
        for unwanted in soup.find_all(['nav', 'header', 'footer', 'aside']):
            unwanted.decompose()
        
        # Entferne Skripte und Style-Elemente
        for script in soup.find_all(['script', 'style']):
            script.decompose()
        
        # Versuche verschiedene Container f√ºr den Haupttext
        main_text = None
        
        # Suche nach dem Hauptcontent
        for selector in ['main', '.content', '#content', '.text', '.work-content']:
            element = soup.select_one(selector)
            if element:
                main_text = element.get_text()
                break
        
        # Fallback: Ganzer Body-Text
        if not main_text:
            # Entferne bekannte Navigations-Elemente
            for unwanted in soup.find_all(class_=['nav', 'navbar', 'menu', 'sidebar']):
                unwanted.decompose()
            
            main_text = soup.get_text()
        
        return clean_extracted_text(main_text)
        
    except Exception as e:
        print(f"     ‚ùå Text-Extraktion fehlgeschlagen: {e}")
        return ""

def clean_extracted_text(text):
    """Bereinigt extrahierten Text"""
    if not text:
        return ""
    
    # Entferne Navigation und Metadaten
    lines_to_remove = [
        r'.*navigation.*',
        r'.*sprache.*',
        r'.*copyright.*',
        r'.*impressum.*',
        r'.*datenschutz.*',
        r'.*cookies.*',
        r'.*start.*werke.*suche.*',
        r'^de\s*en\s*fr$',
        r'.*unifr\.ch.*',
        r'.*resultate filtern.*',
        r'.*schlagw√∂rter.*',
        r'.*gregor emmenegger.*',
        r'.*theologische fakult√§t.*',
        r'.*patristik.*'
    ]
    
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        
        # √úberspringe leere Zeilen
        if not line:
            continue
        
        # √úberspringe Navigations-/Metadaten-Zeilen
        skip_line = False
        for pattern in lines_to_remove:
            if re.match(pattern, line, re.IGNORECASE):
                skip_line = True
                break
        
        if not skip_line and len(line) > 10:
            cleaned_lines.append(line)
    
    # F√ºge zusammen und bereinige
    cleaned_text = '\n'.join(cleaned_lines)
    
    # Entferne √ºberm√§√üige Leerzeichen
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
    cleaned_text = re.sub(r'\n\s*\n', '\n', cleaned_text)
    
    return cleaned_text.strip()

def create_supabase_entries(text, author_name, work_title):
    """Erstellt Supabase-Eintr√§ge aus Text"""
    if not text or len(text) < 200:
        return []
    
    # Teile Text in sinnvolle Abschnitte (ca. 200-400 W√∂rter)
    paragraphs = text.split('\n')
    
    entries = []
    current_section = ""
    section_num = 1
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        
        if len(paragraph) < 20:  # √úberspringe sehr kurze Abs√§tze
            continue
        
        # Sammle Text f√ºr aktuellen Abschnitt
        if current_section:
            current_section += " " + paragraph
        else:
            current_section = paragraph
        
        # Wenn Abschnitt gro√ü genug ist, speichere ihn
        word_count = len(current_section.split())
        if word_count >= 200:
            
            # Erstelle Eintrag
            entry_id = create_unique_id(author_name, work_title, section_num)
            entries.append({
                'id': entry_id,
                'author': author_name,
                'work_title': work_title,
                'section': section_num,
                'text': current_section.strip(),
                'word_count': word_count,
                'language': 'de'
            })
            
            # Reset f√ºr n√§chsten Abschnitt
            current_section = ""
            section_num += 1
    
    # Letzten Abschnitt hinzuf√ºgen falls gro√ü genug
    if current_section and len(current_section.split()) >= 50:
        entry_id = create_unique_id(author_name, work_title, section_num)
        entries.append({
            'id': entry_id,
            'author': author_name,
            'work_title': work_title,
            'section': section_num,
            'text': current_section.strip(),
            'word_count': len(current_section.split()),
            'language': 'de'
        })
    
    return entries

def create_unique_id(author, work_title, section):
    """Erstellt eindeutige ID"""
    author_clean = re.sub(r'[^a-zA-Z0-9]', '_', author)
    work_clean = re.sub(r'[^a-zA-Z0-9]', '_', work_title)
    
    if len(work_clean) > 30:
        work_clean = work_clean[:30]
    
    return f"{author_clean}_{work_clean}_{section}"

def save_author_csv(author_name, entries):
    """Speichert CSV f√ºr einen Autor"""
    if not entries:
        return False
    
    filename = f"supabase_{author_name.replace(' ', '_')}.csv"
    df = pd.DataFrame(entries)
    df.to_csv(filename, index=False, encoding='utf-8')
    print(f"   üíæ Gespeichert: {filename} ({len(entries)} Eintr√§ge)")
    return True

def move_csvs_to_target_folder():
    """Verschiebt neue CSV-Dateien in den Zielordner"""
    target_folder = Path("LuxDei/lib/supabase_csvs")
    
    if not target_folder.exists():
        print(f"‚ùå Zielordner {target_folder} existiert nicht!")
        return
    
    # Finde alle supabase_*.csv Dateien im aktuellen Verzeichnis
    csv_files = list(Path(".").glob("supabase_*.csv"))
    
    if not csv_files:
        print("‚ÑπÔ∏è Keine neuen CSV-Dateien gefunden")
        return
    
    moved_count = 0
    for csv_file in csv_files:
        target_path = target_folder / csv_file.name
        
        if not target_path.exists():
            try:
                csv_file.rename(target_path)
                print(f"   üìÅ Verschoben: {csv_file.name} ‚Üí {target_path}")
                moved_count += 1
            except Exception as e:
                print(f"   ‚ùå Fehler beim Verschieben von {csv_file.name}: {e}")
    
    print(f"\nüìä {moved_count} CSV-Dateien verschoben")

def main():
    """Hauptfunktion"""
    print("üöÄ BKV KIRCHENV√ÑTER SPEZIFISCHER SCRAPER")
    print("=" * 60)
    
    # Hole spezifische Autor-Links
    author_links = get_specific_author_links()
    
    if not author_links:
        print("‚ùå Keine Autor-Links definiert!")
        return
    
    successful_authors = 0
    total_entries = 0
    
    # Verarbeite jeden Autor
    for author, author_url in author_links.items():
        try:
            print(f"\n{'='*60}")
            print(f"üîç VERARBEITE: {author}")
            print(f"{'='*60}")
            
            # Hole deutsche Werke
            works = get_german_works_from_author(author, author_url)
            
            if not works:
                print(f"   ‚ùå Keine deutschen Werke gefunden")
                continue
            
            # Verarbeite maximal 3 Werke pro Autor
            author_entries = []
            
            for work in works[:3]:
                print(f"   üìñ Verarbeite: {work['title']}")
                
                text = extract_text_from_work_page(work['url'])
                
                if text and len(text) > 500:
                    entries = create_supabase_entries(text, author, work['title'])
                    author_entries.extend(entries)
                    print(f"     ‚úÖ {len(entries)} Abschnitte erstellt")
                else:
                    print(f"     ‚ùå Zu wenig Text ({len(text)} Zeichen)")
                
                time.sleep(2)  # Rate limiting
            
            # Speichere CSV f√ºr diesen Autor
            if author_entries:
                if save_author_csv(author, author_entries):
                    successful_authors += 1
                    total_entries += len(author_entries)
                    print(f"   üéâ {author}: {len(author_entries)} Eintr√§ge gespeichert")
            
        except Exception as e:
            print(f"   ‚ùå Fehler bei {author}: {e}")
            continue
    
    # Verschiebe CSV-Dateien in Zielordner
    move_csvs_to_target_folder()
    
    # Finale Statistiken
    print(f"\n" + "=" * 60)
    print("üéâ SCRAPING ABGESCHLOSSEN!")
    print("=" * 60)
    print(f"üìä Erfolgreiche Autoren: {successful_authors}")
    print(f"üìä Neue Eintr√§ge: {total_entries}")

if __name__ == "__main__":
    main()
